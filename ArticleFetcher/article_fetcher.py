import requests
import xml.etree.ElementTree as ET
import hashlib
import json
import os
from datetime import datetime

RSS_URL = "https://finance.yahoo.com/news/rssindex"
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}
SEEN_HASHES_FILE = "seen_hashes.json"

# ğŸ§  ê¸°ì‚¬ ë§í¬ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  í•´ì‹œ ìƒì„±
def hash_article(link: str) -> str:
    return hashlib.sha256(link.encode("utf-8")).hexdigest()

# ğŸ§  ê¸°ì¡´ ìˆ˜ì§‘í•œ í•´ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
def load_seen_hashes() -> set:
    if os.path.exists(SEEN_HASHES_FILE):
        with open(SEEN_HASHES_FILE, "r") as f:
            return set(json.load(f))
    return set()

# ğŸ§  ìƒˆë¡œìš´ í•´ì‹œ ì €ì¥
def save_seen_hashes(hashes: set):
    with open(SEEN_HASHES_FILE, "w") as f:
        json.dump(list(hashes), f, indent=2)

# ğŸ§  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ë¡œ ì €ì¥
def save_articles_to_json(articles: list):
    today = datetime.now().strftime("%Y%m%d")
    file_name = f"articles_{today}.json"
    with open(file_name, "w", encoding="utf-8") as f:
        json.dump(articles, f, indent=2, ensure_ascii=False)
    print(f"[OK] Saved {len(articles)} articles to {file_name}")

# ğŸ§  RSSë¡œë¶€í„° ê¸°ì‚¬ ìˆ˜ì§‘
def fetch_rss_articles() -> list:
    try:
        response = requests.get(RSS_URL, headers=HEADERS)
        response.raise_for_status()
        response.encoding = "utf-8"
        root = ET.fromstring(response.text)

        articles = []
        for item in root.findall(".//item"):
            title = item.findtext("title")
            link = item.findtext("link")
            pub_date = item.findtext("pubDate")

            articles.append({
                "title": title,
                "link": link,
                "published": pub_date
            })
        return articles

    except requests.RequestException as e:
        print("[ERROR] HTTP ìš”ì²­ ì‹¤íŒ¨:", str(e))
    except ET.ParseError as e:
        print("[ERROR] XML íŒŒì‹± ì‹¤íŒ¨:", str(e))
    return []

# ğŸ§  ì¤‘ë³µì„ ì œê±°í•œ ìƒˆ ê¸°ì‚¬ë§Œ í•„í„°ë§
def filter_new_articles(articles: list, seen_hashes: set) -> tuple:
    new_articles = []
    new_hashes = set()

    for article in articles:
        article_hash = hash_article(article["link"])
        if article_hash in seen_hashes:
            continue
        new_articles.append(article)
        new_hashes.add(article_hash)

    return new_articles, new_hashes

# ğŸ§  ì „ì²´ íë¦„ ì œì–´
def main():
    seen_hashes = load_seen_hashes()
    articles = fetch_rss_articles()

    if not articles:
        print("[INFO] No articles found or RSS fetch failed.")
        return

    new_articles, new_hashes = filter_new_articles(articles, seen_hashes)

    if new_articles:
        for a in new_articles:
            print(f"Title: {a['title']}")
            print(f"Link: {a['link']}")
            print(f"Published: {a['published']}")
            print("-" * 80)

        save_articles_to_json(new_articles)
        save_seen_hashes(seen_hashes.union(new_hashes))
    else:
        print("[INFO] No new articles to save.")

# ğŸ” ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    main()